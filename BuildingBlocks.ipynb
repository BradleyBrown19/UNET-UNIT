{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai import *\n",
    "from fastai.vision import *\n",
    "from fastai.callbacks import *\n",
    "from fastai.utils.mem import *\n",
    "from fastai.vision.gan import *\n",
    "from PIL import Image\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data.dataset import TensorDataset\n",
    "import pdb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building Blocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UnetBlock(nn.Module):\n",
    "    \"A quasi-UNet block, using `PixelShuffle_ICNR upsampling`.\"\n",
    "    def __init__(self, up_in_c:int, x_in_c:int, hook:Hook, final_div:bool=True, blur:bool=False, leaky:float=None,\n",
    "                 self_attention:bool=False):\n",
    "        super().__init__()\n",
    "        self.hook = hook\n",
    "        self.shuf = PixelShuffle_ICNR(up_in_c, up_in_c//2, blur=blur, leaky=leaky)\n",
    "        self.bn = batchnorm_2d(x_in_c)\n",
    "        ni = up_in_c//2 + x_in_c\n",
    "        nf = ni if final_div else ni//2\n",
    "        self.conv1 = conv_layer(ni, nf, leaky=leaky)\n",
    "        self.conv2 = conv_layer(nf, nf, leaky=leaky, self_attention=self_attention)\n",
    "        self.relu = relu(leaky=leaky)\n",
    "\n",
    "    def forward(self, up_in:Tensor) -> Tensor:\n",
    "        s = self.hook.stored\n",
    "        up_out = self.shuf(up_in)\n",
    "        ssh = s.shape[-2:]\n",
    "        if ssh != up_out.shape[-2:]:\n",
    "            up_out = F.interpolate(up_out, s.shape[-2:], mode='nearest')\n",
    "        cat_x = self.relu(torch.cat([up_out, self.bn(s)], dim=1))\n",
    "        return self.conv2(self.conv1(cat_x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _get_sfs_idxs(sizes:Sizes) -> List[int]:\n",
    "    \"Get the indexes of the layers where the size of the activation changes.\"\n",
    "    feature_szs = [size[-1] for size in sizes]\n",
    "    sfs_idxs = list(np.where(np.array(feature_szs[:-1]) != np.array(feature_szs[1:]))[0])\n",
    "    if feature_szs[0] != feature_szs[1]: sfs_idxs = [0] + sfs_idxs\n",
    "    return sfs_idxs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UpBlock(nn.Module):\n",
    "    def __init__(self, ni, nf):\n",
    "        super(UpBlock, self).__init__()\n",
    "        self.bn = batchnorm_2d(nf)\n",
    "        self.conv = Conv2dBlock(nf, nf, ks=5, stride=1, norm=\"bn\", activation=\"relu\", padding=2)\n",
    "        self.shuf = PixelShuffle_ICNR(ni, nf, blur=False, leaky=None)\n",
    "        self.relu = nn.ReLU()\n",
    "    \n",
    "    def forward(self, xb, body=None):\n",
    "        up_out = self.shuf(xb)\n",
    "        \n",
    "        if(body is not None):\n",
    "            ssh = body.shape[-2:]\n",
    "            if ssh != up_out.shape[-2:]:\n",
    "                up_out = F.interpolate(up_out, body.shape[-2:], mode='nearest')\n",
    "            up_out = self.relu(up_out+self.bn(body))\n",
    "\n",
    "        xb = self.conv(up_out)\n",
    "        return xb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Conv2dBlock(nn.Module):\n",
    "    def __init__(self, ni, nf, ks, stride, norm, activation, padding=1):\n",
    "        super(Conv2dBlock, self).__init__()\n",
    "        self.pad = nn.ZeroPad2d(padding)\n",
    "        \n",
    "        norm_dim = nf\n",
    "        if norm == 'bn':\n",
    "            self.norm = nn.BatchNorm2d(norm_dim)\n",
    "        elif norm == 'in':\n",
    "            #self.norm = nn.InstanceNorm2d(norm_dim, track_running_stats=True)\n",
    "            self.norm = nn.InstanceNorm2d(norm_dim)\n",
    "        elif norm == 'ln':\n",
    "            self.norm = LayerNorm(norm_dim)\n",
    "        elif norm == 'adain':\n",
    "            self.norm = AdaptiveInstanceNorm2d(norm_dim)\n",
    "        elif norm == 'none':\n",
    "            self.norm = None\n",
    "        \n",
    "        if activation == 'relu':\n",
    "            self.activation = nn.ReLU(inplace=True)\n",
    "        elif activation == 'lrelu':\n",
    "            self.activation = nn.LeakyReLU(0.2, inplace=True)\n",
    "        elif activation == 'prelu':\n",
    "            self.activation = nn.PReLU()\n",
    "        elif activation == 'selu':\n",
    "            self.activation = nn.SELU(inplace=True)\n",
    "        elif activation == 'tanh':\n",
    "            self.activation = nn.Tanh()\n",
    "        elif activation == 'none':\n",
    "            self.activation = None\n",
    "            \n",
    "        self.conv = nn.Conv2d(ni, nf, ks, stride)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.conv(self.pad(x))\n",
    "        if self.norm:\n",
    "            x = self.norm(x)\n",
    "        if self.activation:\n",
    "            x = self.activation(x)\n",
    "        return x "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNorm(nn.Module):\n",
    "    def __init__(self, num_features, eps=1e-5, affine=True):\n",
    "        super(LayerNorm, self).__init__()\n",
    "        self.num_features = num_features\n",
    "        self.affine = affine\n",
    "        self.eps = eps\n",
    "\n",
    "        if self.affine:\n",
    "            self.gamma = nn.Parameter(torch.Tensor(num_features).uniform_())\n",
    "            self.beta = nn.Parameter(torch.zeros(num_features))\n",
    "\n",
    "    def forward(self, x):\n",
    "        shape = [-1] + [1] * (x.dim() - 1)\n",
    "        # print(x.size())\n",
    "        if x.size(0) == 1:\n",
    "            # These two lines run much faster in pytorch 0.4 than the two lines listed below.\n",
    "            mean = x.view(-1).mean().view(*shape)\n",
    "            std = x.view(-1).std().view(*shape)\n",
    "        else:\n",
    "            mean = x.view(x.size(0), -1).mean(1).view(*shape)\n",
    "            std = x.view(x.size(0), -1).std(1).view(*shape)\n",
    "\n",
    "        x = (x - mean) / (std + self.eps)\n",
    "\n",
    "        if self.affine:\n",
    "            shape = [1, -1] + [1] * (x.dim() - 2)\n",
    "            x = x * self.gamma.view(*shape) + self.beta.view(*shape)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResBlocks(nn.Module):\n",
    "    def __init__(self, num_blocks, dim, norm='in', activation='relu', padding=1):\n",
    "        super(ResBlocks, self).__init__()\n",
    "        self.model = []\n",
    "        for i in range(num_blocks):\n",
    "            self.model += [ResBlock(dim, norm=norm, activation=activation, padding=padding)]\n",
    "        self.model = nn.Sequential(*self.model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "class ResBlock(nn.Module):\n",
    "    def __init__(self, dim, norm='in', activation='relu', padding=1):\n",
    "        super(ResBlock, self).__init__()\n",
    "        self.model = []\n",
    "        self.model += [Conv2dBlock(dim, dim, 3, 1, norm, activation, padding)]\n",
    "        self.model += [Conv2dBlock(dim, dim, 3, 1, norm, activation, padding)]\n",
    "        self.model = nn.Sequential(*self.model)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.model(x) + x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_and_res(ni, nf): return nn.Sequential(res_block(ni), conv_layer(ni, nf, stride=2, bias=True, use_activ=False, leaky=0.1))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}