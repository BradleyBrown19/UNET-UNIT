{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#default_exp train_loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "#export\n",
    "from fastai import *\n",
    "from fastai.vision import *\n",
    "from fastai.callbacks import *\n",
    "from fastai.utils.mem import *\n",
    "from fastai.vision.gan import *\n",
    "from PIL import Image\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data.dataset import TensorDataset\n",
    "import pdb\n",
    "\n",
    "from pyfiles.losses import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class UNET_UNIT(Learner):\n",
    "    \"A `Learner` suitable for GANs.\"\n",
    "    def __init__(self, data:DataBunch, generator:nn.Module, critic:nn.Module, gen_loss_func:LossFunction,\n",
    "                 crit_loss_func:LossFunction, n_crit=None, n_gen=None, switcher:Callback=None, gen_first:bool=False, switch_eval:bool=True,\n",
    "                 show_img:bool=True, clip:float=None, **learn_kwargs):\n",
    "        gan = GANModule(generator, critic)\n",
    "        loss_func = GANLoss(gen_loss_func, crit_loss_func, gan)\n",
    "        switcher = ifnone(switcher, partial(FixedGANSwitcher, n_crit=n_crit, n_gen=n_gen))\n",
    "        super().__init__(data, gan, loss_func=loss_func, callback_fns=[switcher], **learn_kwargs)\n",
    "        trainer = GANTrainer(self, clip=clip, switch_eval=switch_eval, show_img=show_img)\n",
    "        self.gan_trainer = trainer\n",
    "        self.callbacks.append(trainer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class GANModule(nn.Module):\n",
    "    \"Wrapper around a `generator` and a `critic` to create a GAN.\"\n",
    "    def __init__(self, generator:nn.Module=None, critic:nn.Module=None, gen_mode:bool=True):\n",
    "        super().__init__()\n",
    "        self.gen_mode = gen_mode\n",
    "        if generator: self.generator,self.critic = generator,critic\n",
    "\n",
    "    def forward(self, *args):\n",
    "        return self.generator(*args) if self.gen_mode else self.critic(*args)\n",
    "\n",
    "    def switch(self, gen_mode:bool=None):\n",
    "        \"Put the model in generator mode if `gen_mode`, in critic mode otherwise.\"\n",
    "        self.gen_mode = (not self.gen_mode) if gen_mode is None else gen_mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class GANLoss(GANModule):\n",
    "    \"Wrapper around `loss_funcC` (for the critic) and `loss_funcG` (for the generator).\"\n",
    "    def __init__(self, loss_funcG:Callable, loss_funcC:Callable, gan_model:GANModule):\n",
    "        super().__init__()\n",
    "        self.loss_funcG,self.loss_funcC,self.gan_model = loss_funcG,loss_funcC,gan_model\n",
    "\n",
    "    def generator(self, output, x_a, x_b):\n",
    "        \"Evaluate the `output` with the critic then uses `self.loss_funcG` to combine it with `target`.\"\n",
    "        output = torch.split(output, 2, dim=0)\n",
    "        x_a_recon, x_b_recon = torch.split(output[0], 1, dim=0)\n",
    "        x_ab, x_ba = torch.split(output[1], 1, dim=0)\n",
    "        fake_pred_x_aa, fake_pred_x_bb = self.gan_model.critic(x_a_recon, x_b_recon)\n",
    "        fake_pred_x_ab, fake_pred_x_ba = self.gan_model.critic(x_ab, x_ba)\n",
    "        \n",
    "        cycled_output = self.gan_model.generator(x_ba, x_ab)\n",
    "        cycle_a = cycled_output[3]\n",
    "        cycle_b = cycled_output[2]\n",
    "        return self.loss_funcG(x_a, x_b, x_a_recon, x_b_recon, cycle_a, cycle_b, fake_pred_x_ab, fake_pred_x_ba)\n",
    "\n",
    "    def critic(self, real_pred, b, c):\n",
    "        fake = self.gan_model.generator(b.requires_grad_(False), c.requires_grad_(False)).requires_grad_(True)\n",
    "        fake = torch.split(fake, 2, dim=0)\n",
    "        fake_ns = torch.split(fake[0], 1, dim=0)\n",
    "        fake_s = torch.split(fake[1], 1, dim=0)\n",
    "        fake_pred_aToA, fake_pred_bToB = self.gan_model.critic(fake_ns[0], fake_ns[1])\n",
    "        fake_pred_aToB, fake_pred_bToA = self.gan_model.critic(fake_s[0], fake_s[1])\n",
    "        return self.loss_funcC(real_pred[0], real_pred[1], fake_pred_aToA, fake_pred_bToB, fake_pred_aToB, fake_pred_bToA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class GANTrainer(LearnerCallback):\n",
    "    \"Handles GAN Training.\"\n",
    "    _order=-20\n",
    "    def __init__(self, learn:Learner, switch_eval:bool=False, clip:float=None, beta:float=0.98, gen_first:bool=False,\n",
    "                 show_img:bool=True):\n",
    "        super().__init__(learn)\n",
    "        self.switch_eval,self.clip,self.beta,self.gen_first,self.show_img = switch_eval,clip,beta,gen_first,show_img\n",
    "        self.generator,self.critic = self.model.generator,self.model.critic\n",
    "\n",
    "    def _set_trainable(self):\n",
    "        train_model = self.generator if     self.gen_mode else self.critic\n",
    "        loss_model  = self.generator if not self.gen_mode else self.critic\n",
    "        requires_grad(train_model, True)\n",
    "        requires_grad(loss_model, False)\n",
    "        if self.switch_eval:\n",
    "            train_model.train()\n",
    "            loss_model.eval()\n",
    "\n",
    "    def on_train_begin(self, **kwargs):\n",
    "        \"Create the optimizers for the generator and critic if necessary, initialize smootheners.\"\n",
    "        if not getattr(self,'opt_gen',None):\n",
    "            self.opt_gen = self.opt.new([nn.Sequential(*flatten_model(self.generator))])\n",
    "        else: self.opt_gen.lr,self.opt_gen.wd = self.opt.lr,self.opt.wd\n",
    "        if not getattr(self,'opt_critic',None):\n",
    "            self.opt_critic = self.opt.new([nn.Sequential(*flatten_model(self.critic))])\n",
    "        else: self.opt_critic.lr,self.opt_critic.wd = self.opt.lr,self.opt.wd\n",
    "        self.gen_mode = self.gen_first\n",
    "        self.switch(self.gen_mode)\n",
    "        self.closses,self.glosses = [],[]\n",
    "        self.smoothenerG,self.smoothenerC = SmoothenValue(self.beta),SmoothenValue(self.beta)\n",
    "        self.recorder.add_metric_names(['gen_loss', 'disc_loss'])\n",
    "        self.imgs,self.titles = [],[]\n",
    "\n",
    "    def on_train_end(self, **kwargs):\n",
    "        \"Switch in generator mode for showing results.\"\n",
    "        self.switch(gen_mode=True)\n",
    "\n",
    "    def on_batch_begin(self, last_input, last_target, **kwargs):\n",
    "        \"Clamp the weights with `self.clip` if it's not None, return the correct input.\"\n",
    "        if self.gen_mode:\n",
    "            self.last_input = last_input\n",
    "            \n",
    "        if self.clip is not None:\n",
    "            for p in self.critic.parameters(): p.data.clamp_(-self.clip, self.clip)\n",
    "        test = {'last_input':last_input,'last_target':last_input}\n",
    "        #print(test)\n",
    "        return test\n",
    "    \n",
    "    def on_backward_begin(self, last_loss, last_output, **kwargs):\n",
    "        \"Record `last_loss` in the proper list.\"\n",
    "        last_loss = last_loss.detach().cpu()\n",
    "        if self.gen_mode:\n",
    "            self.smoothenerG.add_value(last_loss)\n",
    "            self.glosses.append(self.smoothenerG.smooth)\n",
    "            self.last_gen = last_output.detach().cpu()\n",
    "            last_gen_split = torch.split(self.last_gen, 1, 0)\n",
    "            self.last_critic_preds_ns = self.gan_trainer.critic(last_gen_split[0].cuda(), last_gen_split[1].cuda())\n",
    "            self.last_critic_preds_s = self.gan_trainer.critic(last_gen_split[2].cuda(), last_gen_split[3].cuda())\n",
    "        else:\n",
    "            self.smoothenerC.add_value(last_loss)\n",
    "            self.closses.append(self.smoothenerC.smooth)\n",
    "\n",
    "    def on_epoch_begin(self, epoch, **kwargs):\n",
    "        \"Put the critic or the generator back to eval if necessary.\"\n",
    "        self.switch(self.gen_mode)\n",
    "\n",
    "    def on_epoch_end(self, pbar, epoch, last_metrics, **kwargs):\n",
    "        \"Put the various losses in the recorder and show a sample image.\"\n",
    "        if not hasattr(self, 'last_gen') or not self.show_img: return\n",
    "        data = self.learn.data\n",
    "        inputBPre = torch.unbind(self.last_input[1], dim=0)\n",
    "        aToA = im.Image(self.last_gen[0]/2+0.5)\n",
    "        bToB = im.Image(self.last_gen[1]/2+0.5)\n",
    "        aToB = im.Image(self.last_gen[2]/2+0.5)\n",
    "        bToA = im.Image(self.last_gen[3]/2+0.5)\n",
    "        self.imgs.append(aToA)\n",
    "        self.imgs.append(aToB)\n",
    "        self.imgs.append(bToB)\n",
    "        self.imgs.append(bToA)\n",
    "        self.titles.append(f'Epoch {epoch}-A to A')\n",
    "        self.titles.append(f'Epoch {epoch}-A to B')\n",
    "        self.titles.append(f'Epoch {epoch}-B to B')\n",
    "        self.titles.append(f'Epoch {epoch}-B to A')\n",
    "        pbar.show_imgs(self.imgs, self.titles)\n",
    "        return add_metrics(last_metrics, [getattr(self.smoothenerG,'smooth',None),getattr(self.smoothenerC,'smooth',None)])\n",
    "\n",
    "    def switch(self, gen_mode:bool=None):\n",
    "        \"Switch the model, if `gen_mode` is provided, in the desired mode.\"\n",
    "        self.gen_mode = (not self.gen_mode) if gen_mode is None else gen_mode\n",
    "        self.opt.opt = self.opt_gen.opt if self.gen_mode else self.opt_critic.opt\n",
    "        self._set_trainable()\n",
    "        self.model.switch(gen_mode)\n",
    "        self.loss_func.switch(gen_mode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class FixedGANSwitcher(LearnerCallback):\n",
    "    \"Switcher to do `n_crit` iterations of the critic then `n_gen` iterations of the generator.\"\n",
    "    def __init__(self, learn:Learner, n_crit=5, n_gen=1):\n",
    "        super().__init__(learn)\n",
    "        self.n_crit,self.n_gen = 1,1\n",
    "\n",
    "    def on_train_begin(self, **kwargs):\n",
    "        \"Initiate the iteration counts.\"\n",
    "        self.n_c,self.n_g = 0,0\n",
    "\n",
    "    def on_batch_end(self, iteration, **kwargs):\n",
    "        \"Switch the model if necessary.\"\n",
    "        if self.learn.gan_trainer.gen_mode:\n",
    "            self.n_g += 1\n",
    "            n_iter,n_in,n_out = self.n_gen,self.n_c,self.n_g\n",
    "        else:\n",
    "            self.n_c += 1\n",
    "            n_iter,n_in,n_out = self.n_crit,self.n_g,self.n_c\n",
    "        target = n_iter if isinstance(n_iter, int) else n_iter(n_in)\n",
    "        if target == n_out:\n",
    "            self.learn.gan_trainer.switch()\n",
    "            self.n_c,self.n_g = 0,0"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
