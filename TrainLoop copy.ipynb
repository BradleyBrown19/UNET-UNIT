{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai import *\n",
    "from fastai.vision import *\n",
    "from fastai.callbacks import *\n",
    "from fastai.utils.mem import *\n",
    "from fastai.vision.gan import *\n",
    "from PIL import Image\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data.dataset import TensorDataset\n",
    "import pdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class disc_loss(nn.Module):\n",
    "    #a is 0 and b is 1 for predictions\n",
    "    def forward(self, real_pred_a, real_pred_b, aToA, bToB, aToB, bToA):\n",
    "        loss = 0\n",
    "        #Real Image Predictions\n",
    "        loss += F.mse_loss(real_pred_a, real_pred_a.new_zeros(*real_pred_a.size()))\n",
    "\n",
    "        loss += F.mse_loss(real_pred_b, real_pred_b.new_zeros(*real_pred_b.size()))\n",
    "\n",
    "        #Translated Predictions\n",
    "        loss += F.mse_loss(aToB, aToB.new_zeros(*aToB.size()))\n",
    "\n",
    "        loss += F.mse_loss(bToA, bToA.new_ones(*bToA.size()))\n",
    "        \n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class gen_loss(nn.Module):\n",
    "    \n",
    "    def content_similar(self, input, target):\n",
    "        return F.l1_loss(input, target)*(10)\n",
    "    \n",
    "    def should_look_like_a(self, input_fake_pred):\n",
    "        target = input_fake_pred.new_zeros(*input_fake_pred.size())\n",
    "        return F.mse_loss(input_fake_pred, target)\n",
    "    \n",
    "    def should_look_like_b(self, input_fake_pred):\n",
    "        target = input_fake_pred.new_ones(*input_fake_pred.size())\n",
    "        return F.mse_loss(input_fake_pred, target)\n",
    "    \n",
    "    def forward(self, x_a, x_b, x_a_recon, x_b_recon, x_a_cycled, x_b_cycled, fake_pred_x_ab, fake_pred_x_ba):\n",
    "        loss = 0\n",
    "        x_a, x_b, x_a_recon, x_b_recon = torch.unbind(x_a, dim=0)[0], torch.unbind(x_b, dim=0)[0], torch.unbind(x_a_recon, dim=0)[0], torch.unbind(x_a_recon, dim=0)[0]\n",
    "        \n",
    "        loss += self.should_look_like_a(fake_pred_x_ba)\n",
    "        loss += self.should_look_like_b(fake_pred_x_ab)\n",
    "        \n",
    "        loss += self.content_similar(x_a, x_a_recon)*(0.5)\n",
    "        loss += self.content_similar(x_b, x_b_recon)*(0.5)\n",
    "        \n",
    "        loss += self.content_similar(x_a, x_a_cycled)\n",
    "        loss += self.content_similar(x_b, x_b_cycled)\n",
    "        return loss"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6.7 64-bit ('env': venv)",
   "language": "python",
   "name": "python36764bitenvvenve385314bdbaf49768a526aa0724e1e15"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}